{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMTF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keenandrea/tensorflow2.0/blob/master/LSTMTF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_q753yPwXDW",
        "colab_type": "code",
        "outputId": "b7afcdc1-4e45-404a-f8a4-cbdd1ef71a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-beta0\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-beta0 in /usr/local/lib/python3.6/dist-packages (2.0.0b0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (3.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.1.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.11.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.33.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.16.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.8.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.14.0a20190603)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgBwNH5WwqWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxK6h6C9w9hB",
        "colab_type": "code",
        "outputId": "7142ae4d-06b0-48e2-fdec-6cbec85d476d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open('metakaf.txt', 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 121108 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DB5aCmIxRht",
        "colab_type": "code",
        "outputId": "142209a8-346f-43b3-aba8-5aaabce3238a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One morning, when Gregor Samsa woke from troubled dreams, he found\r\n",
            "himself transformed in his bed into a horrible vermin.  He lay on\r\n",
            "his armour-like back, and if he lifted his head a little he could\r\n",
            "see his brown belly, slightly domed and divided \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YKjmaJuxYid",
        "colab_type": "code",
        "outputId": "685a207f-561a-488c-e401-436230456234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8r_eP2Ax0mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBv06yoZyG9O",
        "colab_type": "code",
        "outputId": "9686b88a-d2ba-4c8d-c904-3e4d1e91f753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  \"'\" :   5,\n",
            "  '(' :   6,\n",
            "  ')' :   7,\n",
            "  ',' :   8,\n",
            "  '-' :   9,\n",
            "  '.' :  10,\n",
            "  ':' :  11,\n",
            "  ';' :  12,\n",
            "  '?' :  13,\n",
            "  'A' :  14,\n",
            "  'B' :  15,\n",
            "  'C' :  16,\n",
            "  'D' :  17,\n",
            "  'E' :  18,\n",
            "  'F' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEF1TNvezWw9",
        "colab_type": "code",
        "outputId": "4f223d45-5486-4c0e-ff89-ef8eeb7d1af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'One morning, ' ---- characters mapped to int ---- > [27 49 40  2 48 50 53 49 44 49 42  8  2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLxo6VSszXIi",
        "colab_type": "code",
        "outputId": "7130a9a8-138a-4acd-f56f-f476bad5d838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "#could not get following loop to work\n",
        "#because tf.enable_eager_execution is not \n",
        "#viable on 2.0\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\n",
            "n\n",
            "e\n",
            " \n",
            "m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agFfBXUCzgIo",
        "colab_type": "code",
        "outputId": "102536ba-c679-4d5f-d9e2-e1299f47bc16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'One morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed in'\n",
            "'to a horrible vermin.  He lay on\\r\\nhis armour-like back, and if he lifted his head a little he could\\r\\n'\n",
            "'see his brown belly, slightly domed and divided by arches into stiff\\r\\nsections.  The bedding was hard'\n",
            "'ly able to cover it and seemed ready\\r\\nto slide off any moment.  His many legs, pitifully thin compare'\n",
            "'d\\r\\nwith the size of the rest of him, waved about helplessly as he\\r\\nlooked.\\r\\n\\r\\n\"What\\'s happened to me?'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht46Z97Jzmgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w412-JBAzqvQ",
        "colab_type": "code",
        "outputId": "7183f29a-c3f9-4776-9180-9caadaea3a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'One morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed i'\n",
            "Target data: 'ne morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed in'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXnNEGb7zsl3",
        "colab_type": "code",
        "outputId": "03d49756-8703-40ad-e65e-015879e6a52f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 27 ('O')\n",
            "  expected output: 49 ('n')\n",
            "Step    1\n",
            "  input: 49 ('n')\n",
            "  expected output: 40 ('e')\n",
            "Step    2\n",
            "  input: 40 ('e')\n",
            "  expected output: 2 (' ')\n",
            "Step    3\n",
            "  input: 2 (' ')\n",
            "  expected output: 48 ('m')\n",
            "Step    4\n",
            "  input: 48 ('m')\n",
            "  expected output: 50 ('o')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urjIob4fz0Wd",
        "colab_type": "code",
        "outputId": "325f6184-6447-46ab-c4a8-566accc05bf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfA4Rw1Vz4gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PhzDoHgz77_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FMcGGaQz_1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5vIv1SG0ATo",
        "colab_type": "code",
        "outputId": "353af20d-eea9-4a5d-94cb-1c9297d706a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 62) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGBYLKS70DjH",
        "colab_type": "code",
        "outputId": "71ea9696-23fe-4937-fe52-1be0fe981634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (64, None, 256)           15872     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (64, None, 62)            63550     \n",
            "=================================================================\n",
            "Total params: 5,326,398\n",
            "Trainable params: 5,326,398\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrUFeIyr0Few",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40IHGEKG0Ic1",
        "colab_type": "code",
        "outputId": "bd831a8f-fdc2-4d20-a53a-07c89e9d8d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2, 23, 61, 26, 49, 48,  2, 17, 27, 14, 27, 21,  8, 57, 43, 42, 24,\n",
              "       18, 56,  4, 56, 44, 31, 27, 47, 30,  6,  8, 50, 57, 30, 43, 56, 29,\n",
              "       19, 55, 35, 41, 21, 59, 34, 31, 51, 21, 27, 13, 52, 53,  5, 10, 11,\n",
              "       61, 19, 24,  1, 18, 16, 38, 43, 23, 47, 56, 10, 34, 23, 38, 33, 43,\n",
              "       37, 16, 18, 58, 38, 45, 52, 40, 61, 30, 58, 28, 46, 56, 30, 11, 55,\n",
              "       24,  3, 10,  9,  6, 34, 47,  2, 58, 30, 48, 32, 16, 31, 37])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRqW1miv0LGV",
        "colab_type": "code",
        "outputId": "7b23a2a8-b79b-4c8d-9f44-fb4ef76c1f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'were.  Now and then he stood up from the table and took\\r\\nsome receipt or document from the little ca'\n",
            "\n",
            "Next Char Predictions: \n",
            " ' JzNnm DOAOH,vhgLEu\"uiTOlS(,ovShuQFtYfHxWTpHO?qr\\'.:zFL\\rECchJlu.WJcVhbCEwcjqezSwPkuS:tL!.-(Wl wSmUCTb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv6hAFaU0N2p",
        "colab_type": "code",
        "outputId": "2d8b438a-be39-41be-89a9-45e679b920df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 62)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1260643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmcsJF_w0V35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4rrDPfr1WP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir training_checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8MAhuFs0ZPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEn9gKo10bG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toCRCe2q0i0I",
        "colab_type": "code",
        "outputId": "16c337ec-1ff6-4fce-b762-f327c6e60697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "18/18 [==============================] - 5s 298ms/step - loss: 2.9577\n",
            "Epoch 2/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 2.7216\n",
            "Epoch 3/100\n",
            "18/18 [==============================] - 5s 293ms/step - loss: 2.4060\n",
            "Epoch 4/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 2.2595\n",
            "Epoch 5/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 2.1626\n",
            "Epoch 6/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 2.0659\n",
            "Epoch 7/100\n",
            "18/18 [==============================] - 5s 292ms/step - loss: 1.9818\n",
            "Epoch 8/100\n",
            "18/18 [==============================] - 5s 296ms/step - loss: 1.9022\n",
            "Epoch 9/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 1.8265\n",
            "Epoch 10/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 1.7556\n",
            "Epoch 11/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 1.6909\n",
            "Epoch 12/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 1.6271\n",
            "Epoch 13/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 1.5679\n",
            "Epoch 14/100\n",
            "18/18 [==============================] - 5s 286ms/step - loss: 1.5122\n",
            "Epoch 15/100\n",
            "18/18 [==============================] - 5s 285ms/step - loss: 1.4579\n",
            "Epoch 16/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 1.4035\n",
            "Epoch 17/100\n",
            "18/18 [==============================] - 6s 317ms/step - loss: 1.3553\n",
            "Epoch 18/100\n",
            "18/18 [==============================] - 5s 285ms/step - loss: 1.3055\n",
            "Epoch 19/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 1.2540\n",
            "Epoch 20/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 1.2027\n",
            "Epoch 21/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 1.1578\n",
            "Epoch 22/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 1.1106\n",
            "Epoch 23/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 1.0617\n",
            "Epoch 24/100\n",
            "18/18 [==============================] - 5s 285ms/step - loss: 1.0198\n",
            "Epoch 25/100\n",
            "18/18 [==============================] - 5s 285ms/step - loss: 0.9724\n",
            "Epoch 26/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 0.9188\n",
            "Epoch 27/100\n",
            "18/18 [==============================] - 5s 284ms/step - loss: 0.8647\n",
            "Epoch 28/100\n",
            "18/18 [==============================] - 5s 293ms/step - loss: 0.8048\n",
            "Epoch 29/100\n",
            "18/18 [==============================] - 5s 297ms/step - loss: 0.7533\n",
            "Epoch 30/100\n",
            "18/18 [==============================] - 5s 285ms/step - loss: 0.7055\n",
            "Epoch 31/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.6664\n",
            "Epoch 32/100\n",
            "18/18 [==============================] - 5s 281ms/step - loss: 0.6191\n",
            "Epoch 33/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 0.5691\n",
            "Epoch 34/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.5280\n",
            "Epoch 35/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 0.4766\n",
            "Epoch 36/100\n",
            "18/18 [==============================] - 5s 286ms/step - loss: 0.4343\n",
            "Epoch 37/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 0.3963\n",
            "Epoch 38/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 0.3523\n",
            "Epoch 39/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 0.3147\n",
            "Epoch 40/100\n",
            "18/18 [==============================] - 5s 301ms/step - loss: 0.2828\n",
            "Epoch 41/100\n",
            "18/18 [==============================] - 5s 286ms/step - loss: 0.2486\n",
            "Epoch 42/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.2185\n",
            "Epoch 43/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.1939\n",
            "Epoch 44/100\n",
            "18/18 [==============================] - 6s 309ms/step - loss: 0.1671\n",
            "Epoch 45/100\n",
            "18/18 [==============================] - 5s 292ms/step - loss: 0.1451\n",
            "Epoch 46/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 0.1293\n",
            "Epoch 47/100\n",
            "18/18 [==============================] - 5s 285ms/step - loss: 0.1143\n",
            "Epoch 48/100\n",
            "18/18 [==============================] - 5s 284ms/step - loss: 0.0983\n",
            "Epoch 49/100\n",
            "18/18 [==============================] - 5s 292ms/step - loss: 0.0876\n",
            "Epoch 50/100\n",
            "18/18 [==============================] - 5s 294ms/step - loss: 0.0792\n",
            "Epoch 51/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 0.0722\n",
            "Epoch 52/100\n",
            "18/18 [==============================] - 5s 295ms/step - loss: 0.0658\n",
            "Epoch 53/100\n",
            "18/18 [==============================] - 5s 293ms/step - loss: 0.0615\n",
            "Epoch 54/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 0.0571\n",
            "Epoch 55/100\n",
            "18/18 [==============================] - 5s 286ms/step - loss: 0.0512\n",
            "Epoch 56/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 0.0440\n",
            "Epoch 57/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.0387\n",
            "Epoch 58/100\n",
            "18/18 [==============================] - 5s 293ms/step - loss: 0.0335\n",
            "Epoch 59/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 0.0287\n",
            "Epoch 60/100\n",
            "18/18 [==============================] - 5s 297ms/step - loss: 0.0246\n",
            "Epoch 61/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 0.0216\n",
            "Epoch 62/100\n",
            "18/18 [==============================] - 5s 294ms/step - loss: 0.0194\n",
            "Epoch 63/100\n",
            "18/18 [==============================] - 5s 283ms/step - loss: 0.0176\n",
            "Epoch 64/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0163\n",
            "Epoch 65/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 0.0152\n",
            "Epoch 66/100\n",
            "18/18 [==============================] - 5s 293ms/step - loss: 0.0141\n",
            "Epoch 67/100\n",
            "18/18 [==============================] - 5s 295ms/step - loss: 0.0130\n",
            "Epoch 68/100\n",
            "18/18 [==============================] - 5s 292ms/step - loss: 0.0120\n",
            "Epoch 69/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 0.0111\n",
            "Epoch 70/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.0104\n",
            "Epoch 71/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0098\n",
            "Epoch 72/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0093\n",
            "Epoch 73/100\n",
            "18/18 [==============================] - 5s 293ms/step - loss: 0.0088\n",
            "Epoch 74/100\n",
            "18/18 [==============================] - 5s 298ms/step - loss: 0.0083\n",
            "Epoch 75/100\n",
            "18/18 [==============================] - 5s 298ms/step - loss: 0.0078\n",
            "Epoch 76/100\n",
            "18/18 [==============================] - 5s 293ms/step - loss: 0.0074\n",
            "Epoch 77/100\n",
            "18/18 [==============================] - 5s 292ms/step - loss: 0.0070\n",
            "Epoch 78/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 0.0067\n",
            "Epoch 79/100\n",
            "18/18 [==============================] - 5s 287ms/step - loss: 0.0064\n",
            "Epoch 80/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0061\n",
            "Epoch 81/100\n",
            "18/18 [==============================] - 5s 292ms/step - loss: 0.0058\n",
            "Epoch 82/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.0056\n",
            "Epoch 83/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0053\n",
            "Epoch 84/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.0052\n",
            "Epoch 85/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0050\n",
            "Epoch 86/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.0048\n",
            "Epoch 87/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 0.0046\n",
            "Epoch 88/100\n",
            "18/18 [==============================] - 5s 289ms/step - loss: 0.0044\n",
            "Epoch 89/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0043\n",
            "Epoch 90/100\n",
            "18/18 [==============================] - 5s 293ms/step - loss: 0.0041\n",
            "Epoch 91/100\n",
            "18/18 [==============================] - 5s 297ms/step - loss: 0.0040\n",
            "Epoch 92/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.0039\n",
            "Epoch 93/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0037\n",
            "Epoch 94/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0036\n",
            "Epoch 95/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0035\n",
            "Epoch 96/100\n",
            "18/18 [==============================] - 5s 290ms/step - loss: 0.0034\n",
            "Epoch 97/100\n",
            "18/18 [==============================] - 5s 291ms/step - loss: 0.0033\n",
            "Epoch 98/100\n",
            "18/18 [==============================] - 5s 294ms/step - loss: 0.0032\n",
            "Epoch 99/100\n",
            "18/18 [==============================] - 6s 316ms/step - loss: 0.0031\n",
            "Epoch 100/100\n",
            "18/18 [==============================] - 5s 288ms/step - loss: 0.0030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jBYPu480jSs",
        "colab_type": "code",
        "outputId": "880f3575-f7b1-4c49-ef9b-ddf96afb6779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_100'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "418XQaTCaQwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrG9u3CUaSjw",
        "colab_type": "code",
        "outputId": "0ff3b3e7-2466-4c1b-b444-8e092aa9dd35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 256)            15872     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (1, None, 62)             63550     \n",
            "=================================================================\n",
            "Total params: 5,326,398\n",
            "Trainable params: 5,326,398\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSraEQnKaUXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 0.75\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZxmfHTyaY3f",
        "colab_type": "code",
        "outputId": "edc9d2f8-d61e-489f-8e5d-826be7bcdc43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"So it goes \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "So it goes that they had more ffrcousted, and she was clearly with a litely she\r\n",
            "position and the forch had only been able to speak to his sister and thank her\r\n",
            "for all that she had to do for him to ket himself as he looked round in the\r\n",
            "darkness.  He soon made the discovery that he had ne\r\n",
            "ligged the burnith pefforct the whole even in ond comples lay then st all about the houriously soon.\r\n",
            "\r\n",
            "The cis of to way of stubberned f om had and firthen besome side a time and and espeain te\r\n",
            "offece as he falled - and carried e prasen and werk as where they caught for his mother's\r\n",
            "screams.  \"Anna! Ann't\r\n",
            "rowant.  He trought to he penturely with a grand announcement of it before the\r\n",
            "next with it housed for a wairto clere him with lotther and looked around in omening heard as if they had\r\n",
            "looked out of the beds flow under the sheet; he gave up the chance to see his\r\n",
            "mother until later and was simply glad that see what Gregor came back from his business\r\n",
            "tripss, morning in mean mare from his mother was coul\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}